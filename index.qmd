---
title: "Random Forests"
author: "Danielle Novinski"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

This is an introduction to random forests, a machine learning method widely used in a variety of applications. Random forest is an ensemble learning method comprised of multiple decision trees. Decision trees are used for predictions and classification in a variety of artificial intelligence and machine learning tasks. Decision trees divide data by fields, creating subsets called nodes. Cut offs are applied based on statistics. Resampling training data and using multiple trees to reduce bias and variance leads to the random forest approach.[1]

Random forest is a classification and regression ensemble method. Ensemble learning methods use several methods to solve a problem. Diversity among methods is indicated by methods making different errors. This leads to improvements in classifications. Random forest extensions include non-parametric test of significance, weighted voting, dynamic integration, weighted random sampling, online random forest, and genetic algorithms. Applications of random forest include ecology, medicine, astronomy, agriculture, traffic, and bioinformatics.[2]

The authors suggest modifications to the random forest approach regarding how individual trees are built, the construction of datasets, and how individual predictions are coalesced into one prediction. Conditional inference forests are random forests where the predictors of each split are tested for response and allows adjustment for different predictors. 
For parameter tuning, if the number of trees increases with the number of predictors, the randomization process will lead to predictors having a good chance to be selected. Predictor selection is important. Some predictors are not informative. The number of predictors is important in order to moderate the effects of strong and weak predictors. The size of trees is determined by the splitting criteria. The size of the leaves is another parameter wherein good predictors could be overlooked if the leaf size threshold is too small. The authors suggest sampling without replacement to avoid bias, and sampling a certain number of observations from multiple classes. Random forest implementations can easily be handled by R packages including randomForest and party. R is a free open source software package with documentation, and includes statistical methods for comparisions. Random forests have been used in several studies related to biology and medicine. In such applications, there is usually a relationship between response and predictor variables and sometimes strong correlations between predictors. Random forests have helped with prediction, ranking of influencial variables, and identifying variables with interactions. However, the randomization within implementations prompts the question of reproducability of results. [3]

As the size of datasets increases (defined by the number of variables exceeding the number of observations), the performance of statistical methods declines; machine learning methods are preferred. The random forest method is comprised of "weak learners - predictors with low bias and high variance". Random forest error rates are typically good. The study found that random forest is sensitive to tuning parameters such as number of variables selected and number of trees. Other studies have found good performance in areas such as predicting customer churn, fraud detection, identifying bacteria and fish, and identifying eye disease. The default value used to split a node is log2 (N) + 1 or sqrt(N) but this may not be optimal. In addition to adjusting the default split value, the authors suggest using a large number of trees, optimizing the number of variables for node splitting per the experiment, using a proximity matrix (checking to see if two values are on the same node) to account for missing values and outliers, and applying feature selection. [4]

One application of random forests is in the medical field. Lin et al (2019) analyzed a very large dataset in order to determine risk factors for diabetes, a disease affecting 425 million adults where chronic high blood sugar has effects such as damaging organs. This study applied feature selection, where large numbers of attributes are evaluated to identify the most optimal attributes for prediction. The variable importance score was used, as well as ranking the area under curve, in which the highest area under curve is optimal.[5]

Another study focused on prediction of students' major and completion status. Supervised statistical learning models consist of response, which is the result to be predicted, and predictors, which are the factors that contribute to prediction. A classification tree can be constructed to predict new results after being trained on predictors. The classifier can divide the data based on conditions and establish groups with many similar observations. A few measures of variable importance can be used to examine the impact of predictor variables. The Gini decrease importance calculates the average impurity measure for predictor across the tree and is easy to use but less reliable. The permutation decrease importance permutes values of a predictor and re-runs the classifier. If the predictor is important, the accuracy will decrease. Thus, the predictors can be ranked to determine the most influential ones. Beaulac and Rosenthal (2019) found that they were able to predict program completion with 78.84% accuracy and predict choice of major with 47.41% accuracy.[6]  



## Methods

The 

## Analysis and Results

### Data and Vizualisation

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```



### Statistical Modeling

### Conclusion

## References
